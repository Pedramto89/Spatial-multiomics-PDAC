{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bdb3bf8",
   "metadata": {},
   "source": [
    "# Multimodal Deep Learning Pipeline for Spatial Transcriptomics\n",
    "\n",
    "This notebook builds and trains a multimodal deep learning model that integrates spatial transcriptomics data with corresponding histology images.\n",
    "\n",
    "*Author: Pedram Torabian*\n",
    "*Last updated: 2025-04-23*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb70068",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "There is a 'requirement.txt' file in the directory that can be used to install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f3a91",
   "metadata": {},
   "source": [
    "# Ultimate goals:\n",
    "Build a multi‑modal VAE that fuses histology, spatial transcriptomics, and coordinates into a biologically meaningful latent space, can reconstruct or impute any missing modality, and model disease progression from normal pancreas through primary tumor to metastatic niches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f175bc7",
   "metadata": {},
   "source": [
    "## 2. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9937e128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.3\n"
     ]
    }
   ],
   "source": [
    "import squidpy as sq\n",
    "print(sq.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ede85c",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Basic Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fef719a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "\n",
      "=== Basic Information ===\n",
      "AnnData object with n_obs × n_vars = 91496 × 17860\n",
      "\n",
      "=== Spatial Coordinates ===\n",
      "Keys in obsm: ['X_integrated', 'spatial_IU_PDA_HM10', 'spatial_IU_PDA_HM11', 'spatial_IU_PDA_HM12', 'spatial_IU_PDA_HM13', 'spatial_IU_PDA_HM2', 'spatial_IU_PDA_HM2_2', 'spatial_IU_PDA_HM3', 'spatial_IU_PDA_HM4', 'spatial_IU_PDA_HM5', 'spatial_IU_PDA_HM6', 'spatial_IU_PDA_HM8', 'spatial_IU_PDA_HM9', 'spatial_IU_PDA_LNM10', 'spatial_IU_PDA_LNM12', 'spatial_IU_PDA_LNM6', 'spatial_IU_PDA_LNM7', 'spatial_IU_PDA_LNM8', 'spatial_IU_PDA_NP10', 'spatial_IU_PDA_NP11', 'spatial_IU_PDA_NP2', 'spatial_IU_PDA_T1', 'spatial_IU_PDA_T10', 'spatial_IU_PDA_T11', 'spatial_IU_PDA_T12', 'spatial_IU_PDA_T2', 'spatial_IU_PDA_T3', 'spatial_IU_PDA_T4', 'spatial_IU_PDA_T6', 'spatial_IU_PDA_T8', 'spatial_IU_PDA_T9']\n",
      "  - X_integrated: shape (91496, 3000)\n",
      "  - spatial_IU_PDA_HM10: shape (91496, 2)\n",
      "  - spatial_IU_PDA_HM11: shape (91496, 2)\n",
      "  - spatial_IU_PDA_HM12: shape (91496, 2)\n",
      "  - spatial_IU_PDA_HM13: shape (91496, 2)\n",
      "  - spatial_IU_PDA_HM2: shape (91496, 2)\n",
      "  - spatial_IU_PDA_HM2_2: shape (91496, 2)\n",
      "  - spatial_IU_PDA_HM3: shape (91496, 2)\n",
      "  - spatial_IU_PDA_HM4: shape (91496, 2)\n",
      "  - spatial_IU_PDA_HM5: shape (91496, 2)\n",
      "  - spatial_IU_PDA_HM6: shape (91496, 2)\n",
      "  - spatial_IU_PDA_HM8: shape (91496, 2)\n",
      "  - spatial_IU_PDA_HM9: shape (91496, 2)\n",
      "  - spatial_IU_PDA_LNM10: shape (91496, 2)\n",
      "  - spatial_IU_PDA_LNM12: shape (91496, 2)\n",
      "  - spatial_IU_PDA_LNM6: shape (91496, 2)\n",
      "  - spatial_IU_PDA_LNM7: shape (91496, 2)\n",
      "  - spatial_IU_PDA_LNM8: shape (91496, 2)\n",
      "  - spatial_IU_PDA_NP10: shape (91496, 2)\n",
      "  - spatial_IU_PDA_NP11: shape (91496, 2)\n",
      "  - spatial_IU_PDA_NP2: shape (91496, 2)\n",
      "  - spatial_IU_PDA_T1: shape (91496, 2)\n",
      "  - spatial_IU_PDA_T10: shape (91496, 2)\n",
      "  - spatial_IU_PDA_T11: shape (91496, 2)\n",
      "  - spatial_IU_PDA_T12: shape (91496, 2)\n",
      "  - spatial_IU_PDA_T2: shape (91496, 2)\n",
      "  - spatial_IU_PDA_T3: shape (91496, 2)\n",
      "  - spatial_IU_PDA_T4: shape (91496, 2)\n",
      "  - spatial_IU_PDA_T6: shape (91496, 2)\n",
      "  - spatial_IU_PDA_T8: shape (91496, 2)\n",
      "  - spatial_IU_PDA_T9: shape (91496, 2)\n",
      "\n",
      "=== Image Data ===\n",
      "Keys in uns: ['spatial']\n",
      "Potential spatial keys: []\n",
      "\n",
      "=== Spatial Coordinates ===\n",
      "  - spatial_IU_PDA_HM10: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_HM11: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_HM12: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_HM13: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_HM2: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_HM2_2: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_HM3: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_HM4: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_HM5: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_HM6: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_HM8: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_HM9: shape (91496, 2)\n",
      "    - First 5 coordinates: [[16714.  9362.]\n",
      " [ 5483.  7241.]\n",
      " [ 6701. 11244.]\n",
      " [ 8595.  8889.]\n",
      " [14144.  5830.]]\n",
      "  - spatial_IU_PDA_LNM10: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_LNM12: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_LNM6: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_LNM7: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_LNM8: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_NP10: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_NP11: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_NP2: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_T1: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_T10: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_T11: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_T12: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_T2: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_T3: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_T4: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_T6: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_T8: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "  - spatial_IU_PDA_T9: shape (91496, 2)\n",
      "    - First 5 coordinates: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "=== Existing Annotations/Clusters ===\n",
      "Keys in obs: ['IU_PDA_HM9_tissue', 'IU_PDA_HM9_row', 'IU_PDA_HM9_col', 'IU_PDA_HM9_imagerow', 'IU_PDA_HM9_imagecol', 'IU_PDA_HM10_tissue', 'IU_PDA_HM10_row', 'IU_PDA_HM10_col', 'IU_PDA_HM10_imagerow', 'IU_PDA_HM10_imagecol', 'IU_PDA_HM11_tissue', 'IU_PDA_HM11_row', 'IU_PDA_HM11_col', 'IU_PDA_HM11_imagerow', 'IU_PDA_HM11_imagecol', 'IU_PDA_HM12_tissue', 'IU_PDA_HM12_row', 'IU_PDA_HM12_col', 'IU_PDA_HM12_imagerow', 'IU_PDA_HM12_imagecol', 'IU_PDA_HM13_tissue', 'IU_PDA_HM13_row', 'IU_PDA_HM13_col', 'IU_PDA_HM13_imagerow', 'IU_PDA_HM13_imagecol', 'IU_PDA_HM2_tissue', 'IU_PDA_HM2_row', 'IU_PDA_HM2_col', 'IU_PDA_HM2_imagerow', 'IU_PDA_HM2_imagecol', 'IU_PDA_HM3_tissue', 'IU_PDA_HM3_row', 'IU_PDA_HM3_col', 'IU_PDA_HM3_imagerow', 'IU_PDA_HM3_imagecol', 'IU_PDA_HM4_tissue', 'IU_PDA_HM4_row', 'IU_PDA_HM4_col', 'IU_PDA_HM4_imagerow', 'IU_PDA_HM4_imagecol', 'IU_PDA_HM5_tissue', 'IU_PDA_HM5_row', 'IU_PDA_HM5_col', 'IU_PDA_HM5_imagerow', 'IU_PDA_HM5_imagecol', 'IU_PDA_HM6_tissue', 'IU_PDA_HM6_row', 'IU_PDA_HM6_col', 'IU_PDA_HM6_imagerow', 'IU_PDA_HM6_imagecol', 'IU_PDA_HM8_tissue', 'IU_PDA_HM8_row', 'IU_PDA_HM8_col', 'IU_PDA_HM8_imagerow', 'IU_PDA_HM8_imagecol', 'IU_PDA_LNM10_tissue', 'IU_PDA_LNM10_row', 'IU_PDA_LNM10_col', 'IU_PDA_LNM10_imagerow', 'IU_PDA_LNM10_imagecol', 'IU_PDA_LNM12_tissue', 'IU_PDA_LNM12_row', 'IU_PDA_LNM12_col', 'IU_PDA_LNM12_imagerow', 'IU_PDA_LNM12_imagecol', 'IU_PDA_LNM6_tissue', 'IU_PDA_LNM6_row', 'IU_PDA_LNM6_col', 'IU_PDA_LNM6_imagerow', 'IU_PDA_LNM6_imagecol', 'IU_PDA_LNM7_tissue', 'IU_PDA_LNM7_row', 'IU_PDA_LNM7_col', 'IU_PDA_LNM7_imagerow', 'IU_PDA_LNM7_imagecol', 'IU_PDA_LNM8_tissue', 'IU_PDA_LNM8_row', 'IU_PDA_LNM8_col', 'IU_PDA_LNM8_imagerow', 'IU_PDA_LNM8_imagecol', 'IU_PDA_HM2_2_tissue', 'IU_PDA_HM2_2_row', 'IU_PDA_HM2_2_col', 'IU_PDA_HM2_2_imagerow', 'IU_PDA_HM2_2_imagecol', 'IU_PDA_NP10_tissue', 'IU_PDA_NP10_row', 'IU_PDA_NP10_col', 'IU_PDA_NP10_imagerow', 'IU_PDA_NP10_imagecol', 'IU_PDA_NP11_tissue', 'IU_PDA_NP11_row', 'IU_PDA_NP11_col', 'IU_PDA_NP11_imagerow', 'IU_PDA_NP11_imagecol', 'IU_PDA_NP2_tissue', 'IU_PDA_NP2_row', 'IU_PDA_NP2_col', 'IU_PDA_NP2_imagerow', 'IU_PDA_NP2_imagecol', 'IU_PDA_T1_tissue', 'IU_PDA_T1_row', 'IU_PDA_T1_col', 'IU_PDA_T1_imagerow', 'IU_PDA_T1_imagecol', 'IU_PDA_T9_tissue', 'IU_PDA_T9_row', 'IU_PDA_T9_col', 'IU_PDA_T9_imagerow', 'IU_PDA_T9_imagecol', 'IU_PDA_T10_tissue', 'IU_PDA_T10_row', 'IU_PDA_T10_col', 'IU_PDA_T10_imagerow', 'IU_PDA_T10_imagecol', 'IU_PDA_T11_tissue', 'IU_PDA_T11_row', 'IU_PDA_T11_col', 'IU_PDA_T11_imagerow', 'IU_PDA_T11_imagecol', 'IU_PDA_T12_tissue', 'IU_PDA_T12_row', 'IU_PDA_T12_col', 'IU_PDA_T12_imagerow', 'IU_PDA_T12_imagecol', 'IU_PDA_T2_tissue', 'IU_PDA_T2_row', 'IU_PDA_T2_col', 'IU_PDA_T2_imagerow', 'IU_PDA_T2_imagecol', 'IU_PDA_T3_tissue', 'IU_PDA_T3_row', 'IU_PDA_T3_col', 'IU_PDA_T3_imagerow', 'IU_PDA_T3_imagecol', 'IU_PDA_T4_tissue', 'IU_PDA_T4_row', 'IU_PDA_T4_col', 'IU_PDA_T4_imagerow', 'IU_PDA_T4_imagecol', 'IU_PDA_T6_tissue', 'IU_PDA_T6_row', 'IU_PDA_T6_col', 'IU_PDA_T6_imagerow', 'IU_PDA_T6_imagecol', 'IU_PDA_T8_tissue', 'IU_PDA_T8_row', 'IU_PDA_T8_col', 'IU_PDA_T8_imagerow', 'IU_PDA_T8_imagecol']\n",
      "\n",
      "Data exploration complete!\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading the dataset...\")\n",
    "adata = ad.read_h5ad(\"spatial_with_images.h5ad\")\n",
    "\n",
    "# Print basic information\n",
    "print(\"\\n=== Basic Information ===\")\n",
    "print(f\"AnnData object with n_obs × n_vars = {adata.n_obs} × {adata.n_vars}\")\n",
    "\n",
    "# Check for spatial coordinates in obsm\n",
    "print(\"\\n=== Spatial Coordinates ===\")\n",
    "print(f\"Keys in obsm: {list(adata.obsm.keys())}\")\n",
    "for key in adata.obsm.keys():\n",
    "    shape = adata.obsm[key].shape\n",
    "    print(f\"  - {key}: shape {shape}\")\n",
    "\n",
    "# Check for image data in uns\n",
    "print(\"\\n=== Image Data ===\")\n",
    "print(f\"Keys in uns: {list(adata.uns.keys())}\")\n",
    "\n",
    "# Look for spatial information in uns\n",
    "spatial_keys = []\n",
    "for key in adata.uns.keys():\n",
    "    if isinstance(adata.uns[key], dict):\n",
    "        if 'images' in adata.uns[key] or 'image' in adata.uns[key]:\n",
    "            spatial_keys.append(key)\n",
    "            \n",
    "print(f\"Potential spatial keys: {spatial_keys}\")\n",
    "\n",
    "# Inspect first spatial key if available\n",
    "if spatial_keys:\n",
    "    first_key = spatial_keys[0]\n",
    "    print(f\"\\nInspecting uns['{first_key}']:\")\n",
    "    \n",
    "    try:\n",
    "        # This section tries different structures that might exist\n",
    "        if isinstance(adata.uns[first_key], dict):\n",
    "            print(f\"  - Keys: {list(adata.uns[first_key].keys())}\")\n",
    "            \n",
    "            # Check if it's a dictionary with library_id keys\n",
    "            if any(k for k in adata.uns[first_key].keys() if k != 'images' and k != 'scalefactors'):\n",
    "                # Visium-style format with library ID\n",
    "                first_lib_id = [k for k in adata.uns[first_key].keys() \n",
    "                               if k != 'images' and k != 'scalefactors'][0]\n",
    "                print(f\"  - Library ID: {first_lib_id}\")\n",
    "                \n",
    "                if 'images' in adata.uns[first_key][first_lib_id]:\n",
    "                    print(f\"  - Image keys: {list(adata.uns[first_key][first_lib_id]['images'].keys())}\")\n",
    "                \n",
    "                if 'scalefactors' in adata.uns[first_key][first_lib_id]:\n",
    "                    print(f\"  - Scalefactor keys: {list(adata.uns[first_key][first_lib_id]['scalefactors'].keys())}\")\n",
    "            \n",
    "            # Direct images dictionary\n",
    "            elif 'images' in adata.uns[first_key]:\n",
    "                print(f\"  - Image keys: {list(adata.uns[first_key]['images'].keys())}\")\n",
    "                \n",
    "                # Check image shapes\n",
    "                for img_key in adata.uns[first_key]['images'].keys():\n",
    "                    img = adata.uns[first_key]['images'][img_key]\n",
    "                    print(f\"    - {img_key} shape: {img.shape}\")\n",
    "                \n",
    "            if 'scalefactors' in adata.uns[first_key]:\n",
    "                print(f\"  - Scalefactor keys: {list(adata.uns[first_key]['scalefactors'].keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting spatial data: {e}\")\n",
    "\n",
    "# Check for obsm spatial coordinates\n",
    "spatial_coord_keys = [key for key in adata.obsm.keys() \n",
    "                     if 'spatial' in key.lower() or 'coord' in key.lower()]\n",
    "\n",
    "if spatial_coord_keys:\n",
    "    print(\"\\n=== Spatial Coordinates ===\")\n",
    "    for key in spatial_coord_keys:\n",
    "        coords = adata.obsm[key]\n",
    "        print(f\"  - {key}: shape {coords.shape}\")\n",
    "        print(f\"    - First 5 coordinates: {coords[:5]}\")\n",
    "\n",
    "# Check if there are any clustering results\n",
    "print(\"\\n=== Existing Annotations/Clusters ===\")\n",
    "print(f\"Keys in obs: {list(adata.obs.columns)}\")\n",
    "cluster_cols = [col for col in adata.obs.columns if 'cluster' in col.lower() or 'group' in col.lower() or 'type' in col.lower()]\n",
    "if cluster_cols:\n",
    "    print(f\"Potential cluster columns: {cluster_cols}\")\n",
    "    for col in cluster_cols[:2]:  # Show first 2 only\n",
    "        print(f\"  - {col} value counts:\")\n",
    "        print(adata.obs[col].value_counts().head())\n",
    "\n",
    "print(\"\\nData exploration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a3ca4",
   "metadata": {},
   "source": [
    "### To see the spatial object structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41afc99",
   "metadata": {},
   "source": [
    "### list of sample types:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f47b70a",
   "metadata": {},
   "source": [
    "#### HM: Hepatic met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbbd87",
   "metadata": {},
   "source": [
    "#### LNM: Lymph node met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84cdce",
   "metadata": {},
   "source": [
    "#### NP: Normal pancreas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca93e17",
   "metadata": {},
   "source": [
    "#### T: primary tumor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aaa0e2",
   "metadata": {},
   "source": [
    "### To make sure how many samples are having spatial coords and images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36aab976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== libraries with usable image & coords ===\n",
      " • IU_PDA_HM10\n",
      " • IU_PDA_HM11\n",
      " • IU_PDA_HM12\n",
      " • IU_PDA_HM13\n",
      " • IU_PDA_HM2\n",
      " • IU_PDA_HM2_2\n",
      " • IU_PDA_HM3\n",
      " • IU_PDA_HM4\n",
      " • IU_PDA_HM5\n",
      " • IU_PDA_HM6\n",
      " • IU_PDA_HM8\n",
      " • IU_PDA_HM9\n",
      " • IU_PDA_LNM10\n",
      " • IU_PDA_LNM12\n",
      " • IU_PDA_LNM6\n",
      " • IU_PDA_LNM7\n",
      " • IU_PDA_LNM8\n",
      " • IU_PDA_NP10\n",
      " • IU_PDA_NP11\n",
      " • IU_PDA_NP2\n",
      " • IU_PDA_T1\n",
      " • IU_PDA_T10\n",
      " • IU_PDA_T11\n",
      " • IU_PDA_T12\n",
      " • IU_PDA_T2\n",
      " • IU_PDA_T3\n",
      " • IU_PDA_T4\n",
      " • IU_PDA_T6\n",
      " • IU_PDA_T8\n",
      " • IU_PDA_T9\n",
      "\n",
      "=== libraries missing image or coords ===\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "libs_with_image, libs_without_image = [], []\n",
    "\n",
    "def has_image(entry):\n",
    "    \"\"\"Return True if `entry` is a valid path OR an in‑memory image array.\"\"\"\n",
    "    if entry is None:\n",
    "        return False\n",
    "    # 1) path‑like (string or Path)\n",
    "    if isinstance(entry, (str, Path)):\n",
    "        return Path(entry).expanduser().exists()\n",
    "    # 2) ndarray (hires image stored in the AnnData)\n",
    "    if isinstance(entry, (np.ndarray,)):\n",
    "        return entry.size > 0\n",
    "    # 3) PIL image, xarray.DataArray, etc. – just assume it’s valid\n",
    "    return True\n",
    "\n",
    "for lib_id, meta in adata.uns.get(\"spatial\", {}).items():\n",
    "    img_entry = meta.get(\"images\", {}).get(\"hires\")  # or \"lowres\"\n",
    "    img_ok    = has_image(img_entry)\n",
    "\n",
    "    key = f\"spatial_{lib_id}\"\n",
    "    coords_ok = (\n",
    "        key in adata.obsm and\n",
    "        (~np.isnan(adata.obsm[key][:, 0])).any()\n",
    "    )\n",
    "\n",
    "    (libs_with_image if img_ok and coords_ok else libs_without_image).append(lib_id)\n",
    "\n",
    "print(\"=== libraries with usable image & coords ===\")\n",
    "print(\"\\n\".join(f\" • {lib}\" for lib in libs_with_image) or \"None\")\n",
    "\n",
    "print(\"\\n=== libraries missing image or coords ===\")\n",
    "print(\"\\n\".join(f\" • {lib}\" for lib in libs_without_image) or \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ed2d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_slides = libs_with_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f429d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Slide IU_PDA_HM10:\n",
      "  Valid coordinates: 2348 out of 91496\n",
      "  First 3 valid coordinates: [[11750.  5932.]\n",
      " [ 3323.  4360.]\n",
      " [ 2719.  6480.]]\n",
      "  Located at indices: [1908 1909 1910]...\n",
      "\n",
      "Slide IU_PDA_HM11:\n",
      "  Valid coordinates: 3931 out of 91496\n",
      "  First 3 valid coordinates: [[15841. 10096.]\n",
      " [ 4608.  7971.]\n",
      " [14754. 18574.]]\n",
      "  Located at indices: [4256 4257 4258]...\n",
      "\n",
      "Slide IU_PDA_HM12:\n",
      "  Valid coordinates: 2961 out of 91496\n",
      "  First 3 valid coordinates: [[8333. 4588.]\n",
      " [7781. 8827.]\n",
      " [7997. 3291.]]\n",
      "  Located at indices: [8187 8188 8189]...\n",
      "\n",
      "Slide IU_PDA_HM13:\n",
      "  Valid coordinates: 2182 out of 91496\n",
      "  First 3 valid coordinates: [[ 5056.  6369.]\n",
      " [15196. 16963.]\n",
      " [ 3702. 10134.]]\n",
      "  Located at indices: [11148 11149 11150]...\n",
      "\n",
      "Slide IU_PDA_HM2:\n",
      "  Valid coordinates: 2478 out of 91496\n",
      "  First 3 valid coordinates: [[ 8721. 19618.]\n",
      " [15621. 17036.]\n",
      " [ 8736.  3146.]]\n",
      "  Located at indices: [13330 13331 13332]...\n"
     ]
    }
   ],
   "source": [
    "# Find where valid coordinates are located for each slide\n",
    "for slide in unique_slides[:5]:  # Check first 5 slides as example\n",
    "    spatial_key = f'spatial_{slide}'\n",
    "    \n",
    "    # Find indices with valid coordinates\n",
    "    valid_mask = ~np.isnan(adata.obsm[spatial_key]).any(axis=1)\n",
    "    num_valid = np.sum(valid_mask)\n",
    "    \n",
    "    # Get valid coordinates\n",
    "    valid_coords = adata.obsm[spatial_key][valid_mask]\n",
    "    \n",
    "    print(f\"\\nSlide {slide}:\")\n",
    "    print(f\"  Valid coordinates: {num_valid} out of {adata.obsm[spatial_key].shape[0]}\")\n",
    "    if num_valid > 0:\n",
    "        print(f\"  First 3 valid coordinates: {valid_coords[:3]}\")\n",
    "        \n",
    "        # Get indices of valid coordinates\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        print(f\"  Located at indices: {valid_indices[:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbaa995",
   "metadata": {},
   "source": [
    "### List od slide IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c35081cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recovered slide_ids: ['IU_PDA_HM10' 'IU_PDA_HM11' 'IU_PDA_HM12' 'IU_PDA_HM13' 'IU_PDA_HM2'\n",
      " 'IU_PDA_HM2_2' 'IU_PDA_HM3' 'IU_PDA_HM4' 'IU_PDA_HM5' 'IU_PDA_HM6'\n",
      " 'IU_PDA_HM8' 'IU_PDA_HM9' 'IU_PDA_LNM10' 'IU_PDA_LNM12' 'IU_PDA_LNM6'\n",
      " 'IU_PDA_LNM7' 'IU_PDA_LNM8' 'IU_PDA_NP10' 'IU_PDA_NP11' 'IU_PDA_NP2'\n",
      " 'IU_PDA_T1' 'IU_PDA_T10' 'IU_PDA_T11' 'IU_PDA_T12' 'IU_PDA_T2'\n",
      " 'IU_PDA_T3' 'IU_PDA_T4' 'IU_PDA_T6' 'IU_PDA_T8' 'IU_PDA_T9']  (total spots: 91496 )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---- 1. empty vector the size of n_obs ----\n",
    "slide_ids = np.empty(adata.n_obs, dtype=object)\n",
    "\n",
    "# ---- 2. loop over the 30 libraries ----\n",
    "for lib_id in adata.uns[\"spatial\"].keys():          # e.g. \"IU_PDA_T9\"\n",
    "    coords = adata.obsm.get(f\"spatial_{lib_id}\")\n",
    "    if coords is None:               # should not happen\n",
    "        continue\n",
    "    mask = ~np.isnan(coords[:, 0])   # True for spots coming from this slide\n",
    "    slide_ids[mask] = lib_id\n",
    "\n",
    "# ---- 3. sanity check ----\n",
    "assert (slide_ids != None).all(), \"some spots remain unassigned\"\n",
    "print(\"recovered slide_ids:\", np.unique(slide_ids), \" (total spots:\", len(slide_ids), \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "758f305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"slide_id\"] = slide_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181655ed",
   "metadata": {},
   "source": [
    "## 1. Map slides to 0 – 3 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a33db8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP, T, HM, LNM spot counts: [ 9820 35458 28520 17698]\n"
     ]
    }
   ],
   "source": [
    "import re, numpy as np\n",
    "\n",
    "type_map = {\"NP\":0, \"T\":1, \"HM\":2, \"LNM\":3}\n",
    "\n",
    "def tag_to_code(slide_id):\n",
    "    tag = slide_id.split(\"_\")[2]        # \"HM9\", \"T10\", ...\n",
    "    tag = re.match(r\"[A-Z]+\", tag).group()   # keep only the letters: \"HM\"\n",
    "    return type_map[tag]\n",
    "\n",
    "sample_type_vec = np.fromiter(\n",
    "    (tag_to_code(s) for s in slide_ids),\n",
    "    dtype=np.int32,\n",
    "    count=len(slide_ids)\n",
    ")\n",
    "\n",
    "# sanity‑check\n",
    "counts = np.bincount(sample_type_vec, minlength=4)\n",
    "print(\"NP, T, HM, LNM spot counts:\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6b827",
   "metadata": {},
   "source": [
    "### Stage | Accomplished\n",
    "Data plumbing |  All 91 496 spots now have: gene vector, (x , y) coords, 224 × 224 patch, slide‑ID, sample‑type code.\n",
    "Three‑branch encoder |  CNN + MLP + xy network defined.\n",
    "Auxiliary heads |  class_out (tissue label) and type_out (NP / T / HM / LNM) wired in.\n",
    "Training dataset |  tf.data.Dataset streams images from patches.h5 and feeds all five inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76782a4e",
   "metadata": {},
   "source": [
    "### Goal of the next runs: train a group‑aware latent that already tells healthy from tumour, etc. Then decide if I need the generative VAE extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758bb50",
   "metadata": {},
   "source": [
    "### Train the encoder (A shared encoder learns modality fusion (patch + genes + coords) once)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07d2af",
   "metadata": {},
   "source": [
    "### The auxiliary sample‑type head forces the latent to respect large biological classes (healthy vs tumour vs metastasis). That makes downstream clustering much cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99035a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genes_all shape: (91496, 17860)\n",
      "xy_all   shape: (91496, 2)\n"
     ]
    }
   ],
   "source": [
    "# --- create genes_all & xy_all in RAM ---\n",
    "import numpy as np\n",
    "# 1. gene matrix  (log‑transform then float32)\n",
    "genes_all = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n",
    "genes_all = np.log1p(genes_all).astype(np.float32)\n",
    "\n",
    "# 2. spatial coords  (choose the correct key for each spot)\n",
    "xy_all = np.column_stack([adata.obsm[\"X_spatial_x\"], adata.obsm[\"X_spatial_y\"]]) \\\n",
    "         if \"X_spatial_x\" in adata.obsm else adata.obsm[\"spatial_IU_PDA_HM9\"]  # adapt as needed\n",
    "xy_all = xy_all.astype(np.float32)\n",
    "\n",
    "print(\"genes_all shape:\", genes_all.shape)  # should be (91496, 17860)\n",
    "print(\"xy_all   shape:\", xy_all.shape)      # (91496, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cd5fbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique slides: 30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# slide_ids already exists; make it categorical\n",
    "slide_codes = pd.Categorical(slide_ids).codes   # 0 … n_slides‑1\n",
    "n_slides    = slide_codes.max() + 1             # integer count\n",
    "print(\"unique slides:\", n_slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bff69d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU(s)\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla V100-PCIE-16GB, compute capability 7.0\n",
      "Using mixed precision policy\n"
     ]
    }
   ],
   "source": [
    "# Put this at the very beginning of your script after imports\n",
    "# BEFORE any other TensorFlow operations\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set memory growth via environment variable instead\n",
    "# This must be done before importing TensorFlow, but we can try this approach\n",
    "# as an alternative\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "# Check if GPU is available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print(f\"Using {len(physical_devices)} GPU(s)\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")\n",
    "\n",
    "# Try mixed precision without requiring memory growth\n",
    "try:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"Using mixed precision policy\")\n",
    "except:\n",
    "    print(\"Mixed precision not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20085855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, h5py, numpy as np\n",
    "\n",
    "def make_ds(indices, batch=32, shuffle_buffer=10000):\n",
    "    \"\"\"\n",
    "    Build a tf.data.Dataset for a list/array of spot indices.\n",
    "    Streams images from patches.h5 and yields the five‑input dict\n",
    "    plus the two‑head target dict.\n",
    "    \"\"\"\n",
    "\n",
    "    def gen():\n",
    "        # open HDF5 inside generator so each worker gets its own handle\n",
    "        h5 = h5py.File(\"patches.h5\", \"r\")\n",
    "        for i in indices:\n",
    "            slide  = slide_ids[i]\n",
    "            local  = (np.where(slide_ids == slide)[0] == i).nonzero()[0][0]\n",
    "            img    = h5[slide][local].astype(np.float32) / 255.0\n",
    "\n",
    "            inputs  = {\n",
    "                \"img\":   img,\n",
    "                \"gene\":  genes_all[i],\n",
    "                \"xy\":    xy_all[i],\n",
    "                \"slide\": np.array([slide_codes[i]],       dtype=np.int32),\n",
    "                \"sample_type\": np.array([sample_type_vec[i]], dtype=np.int32),\n",
    "            }\n",
    "            targets = {\n",
    "                \"class_out\": y_all[i],\n",
    "                \"type_out\":  tf.one_hot(sample_type_vec[i], 4),\n",
    "            }\n",
    "            yield inputs, targets\n",
    "        h5.close()                 # optional in eager mode\n",
    "\n",
    "    # ----- tensorflow signature -----\n",
    "    out_sig = (\n",
    "        { \"img\":   tf.TensorSpec((224,224,3), tf.float32),\n",
    "          \"gene\":  tf.TensorSpec((genes_all.shape[1],), tf.float32),\n",
    "          \"xy\":    tf.TensorSpec((2,), tf.float32),\n",
    "          \"slide\": tf.TensorSpec((1,), tf.int32),\n",
    "          \"sample_type\": tf.TensorSpec((1,), tf.int32) },\n",
    "        { \"class_out\": tf.TensorSpec((num_classes,), tf.float32),\n",
    "          \"type_out\":  tf.TensorSpec((4,), tf.float32) }\n",
    "    )\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=out_sig)\n",
    "    ds = ds.shuffle(shuffle_buffer).batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c51c73dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_col = [col for col in adata.obs.columns if col.endswith(\"_tissue\")][0]\n",
    "labels_all = adata.obs[tissue_col].astype(\"category\").cat.codes.values\n",
    "num_classes = labels_all.max() + 1\n",
    "import tensorflow as tf\n",
    "y_all = tf.one_hot(labels_all, num_classes).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1ebd53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train spots: 73196, val spots: 18300\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "idx_train, idx_val = train_test_split(\n",
    "    np.arange(len(labels_all)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels_all\n",
    ")\n",
    "\n",
    "print(f\"train spots: {len(idx_train)}, val spots: {len(idx_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab44a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define inputs\n",
    "img_in = Input((224,224,3), name=\"img\")\n",
    "gene_in = Input((genes_all.shape[1],), name=\"gene\")\n",
    "xy_in = Input((2,), name=\"xy\")\n",
    "slide_in = Input((1,), name=\"slide\")\n",
    "st_in = Input((1,), name=\"sample_type\")\n",
    "\n",
    "# Image tower with batch normalization\n",
    "h = layers.Conv2D(32, 3, padding='same')(img_in)\n",
    "h = layers.BatchNormalization()(h)\n",
    "h = layers.Activation(\"relu\")(h)\n",
    "h = layers.MaxPool2D()(h)\n",
    "h = layers.Conv2D(64, 3, padding='same')(h)\n",
    "h = layers.BatchNormalization()(h)\n",
    "h = layers.Activation(\"relu\")(h)\n",
    "h = layers.GlobalAveragePooling2D()(h)\n",
    "\n",
    "# Gene tower with batch normalization\n",
    "g = layers.Dense(512)(gene_in)\n",
    "g = layers.BatchNormalization()(g)\n",
    "g = layers.Activation(\"relu\")(g)\n",
    "g = layers.Dropout(0.3)(g)\n",
    "g = layers.Dense(128)(g)\n",
    "g = layers.BatchNormalization()(g)\n",
    "g = layers.Activation(\"relu\")(g)\n",
    "\n",
    "# Coordinate tower\n",
    "s = layers.Dense(32)(xy_in)\n",
    "s = layers.BatchNormalization()(s)\n",
    "s = layers.Activation(\"relu\")(s)\n",
    "\n",
    "# Slide-ID embedding\n",
    "b = layers.Flatten()(layers.Embedding(n_slides, 8)(slide_in))\n",
    "\n",
    "# Sample-type embedding\n",
    "st = layers.Flatten()(layers.Embedding(4, 4)(st_in))\n",
    "\n",
    "# Fusion layer with batch normalization\n",
    "z = layers.concatenate([h, g, s, b, st])\n",
    "z = layers.BatchNormalization()(z)\n",
    "z = layers.Dense(128)(z)\n",
    "z = layers.BatchNormalization()(z)\n",
    "z = layers.Activation(\"relu\")(z)\n",
    "z_lat = layers.Dense(64, activation=\"relu\", name=\"latent\")(z)\n",
    "\n",
    "# Classification heads\n",
    "class_out = layers.Dense(num_classes, activation=\"softmax\", name=\"class_out\")(z_lat)\n",
    "type_out = layers.Dense(4, activation=\"softmax\", name=\"type_out\")(z_lat)\n",
    "\n",
    "# Assemble model\n",
    "model_robust = Model(\n",
    "    {\"img\": img_in, \"gene\": gene_in, \"xy\": xy_in,\n",
    "     \"slide\": slide_in, \"sample_type\": st_in},\n",
    "    {\"class_out\": class_out, \"type_out\": type_out}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187e270",
   "metadata": {},
   "source": [
    "### Train the current model first to ensure all encoders work properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ca9c777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique slides: 30\n"
     ]
    }
   ],
   "source": [
    "# Create codes for slide_ids\n",
    "unique_slides = np.unique(slide_ids)\n",
    "slide_to_code = {slide: i for i, slide in enumerate(unique_slides)}\n",
    "slide_codes = np.array([slide_to_code[slide] for slide in slide_ids])\n",
    "n_slides = len(unique_slides)\n",
    "\n",
    "print(f\"Number of unique slides: {n_slides}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5732fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds(indices, batch=32, shuffle_buffer=10000):\n",
    "    \"\"\"\n",
    "    Build a tf.data.Dataset for a list/array of spot indices.\n",
    "    Streams images from patches.h5 and yields the five‑input dict\n",
    "    plus the two‑head target dict.\n",
    "    \"\"\"\n",
    "    def gen():\n",
    "        # open HDF5 inside generator so each worker gets its own handle\n",
    "        h5 = h5py.File(\"patches.h5\", \"r\")\n",
    "        for i in indices:\n",
    "            slide = slide_ids[i]\n",
    "            local = (np.where(slide_ids == slide)[0] == i).nonzero()[0][0]\n",
    "            img = h5[slide][local].astype(np.float32) / 255.0\n",
    "            inputs = {\n",
    "                \"img\": img,\n",
    "                \"gene\": genes_all[i],\n",
    "                \"xy\": xy_all[i],\n",
    "                \"slide\": np.array([slide_codes[i]], dtype=np.int32),\n",
    "                \"sample_type\": np.array([sample_type_vec[i]], dtype=np.int32),\n",
    "            }\n",
    "            targets = {\n",
    "                \"class_out\": y_all[i],\n",
    "                \"type_out\": tf.one_hot(sample_type_vec[i], 4),\n",
    "            }\n",
    "            yield inputs, targets\n",
    "        h5.close()\n",
    "        \n",
    "    # ----- tensorflow signature -----\n",
    "    out_sig = (\n",
    "        {\"img\": tf.TensorSpec((224,224,3), tf.float32),\n",
    "         \"gene\": tf.TensorSpec((genes_all.shape[1],), tf.float32),\n",
    "         \"xy\": tf.TensorSpec((2,), tf.float32),\n",
    "         \"slide\": tf.TensorSpec((1,), tf.int32),\n",
    "         \"sample_type\": tf.TensorSpec((1,), tf.int32)},\n",
    "        {\"class_out\": tf.TensorSpec((num_classes,), tf.float32),\n",
    "         \"type_out\": tf.TensorSpec((4,), tf.float32)}\n",
    "    )\n",
    "    \n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=out_sig)\n",
    "    \n",
    "    # Only shuffle if shuffle_buffer is not None\n",
    "    if shuffle_buffer is not None:\n",
    "        ds = ds.shuffle(shuffle_buffer)\n",
    "        \n",
    "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0385bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Image Data in AnnData ===\n",
      "Keys in spatial: ['IU_PDA_HM10', 'IU_PDA_HM11', 'IU_PDA_HM12', 'IU_PDA_HM13', 'IU_PDA_HM2', 'IU_PDA_HM2_2', 'IU_PDA_HM3', 'IU_PDA_HM4', 'IU_PDA_HM5', 'IU_PDA_HM6', 'IU_PDA_HM8', 'IU_PDA_HM9', 'IU_PDA_LNM10', 'IU_PDA_LNM12', 'IU_PDA_LNM6', 'IU_PDA_LNM7', 'IU_PDA_LNM8', 'IU_PDA_NP10', 'IU_PDA_NP11', 'IU_PDA_NP2', 'IU_PDA_T1', 'IU_PDA_T10', 'IU_PDA_T11', 'IU_PDA_T12', 'IU_PDA_T2', 'IU_PDA_T3', 'IU_PDA_T4', 'IU_PDA_T6', 'IU_PDA_T8', 'IU_PDA_T9']\n",
      "\n",
      "First slide: IU_PDA_HM10\n",
      "Keys in first slide: ['images', 'scalefactors']\n",
      "\n",
      "Images found!\n",
      "Image types: ['hires', 'hires_dims', 'hires_source']\n",
      "  hires shape: (590, 600, 3)\n",
      "  Data type: float32\n",
      "  Min/Max values: 0.21568627655506134, 0.9607843160629272\n",
      "  hires_dims shape: (2,)\n",
      "  Data type: int64\n",
      "  Min/Max values: 2000, 2000\n",
      "  hires_source: String value - 'IU_PDA_HM10.png'\n"
     ]
    }
   ],
   "source": [
    "# Examine images in AnnData\n",
    "print(\"\\n=== Image Data in AnnData ===\")\n",
    "if 'spatial' in adata.uns:\n",
    "    print(f\"Keys in spatial: {list(adata.uns['spatial'].keys())}\")\n",
    "    \n",
    "    # Look at first slide\n",
    "    first_slide = list(adata.uns['spatial'].keys())[0]\n",
    "    print(f\"\\nFirst slide: {first_slide}\")\n",
    "    print(f\"Keys in first slide: {list(adata.uns['spatial'][first_slide].keys())}\")\n",
    "    \n",
    "    # Check if there are images\n",
    "    if 'images' in adata.uns['spatial'][first_slide]:\n",
    "        print(\"\\nImages found!\")\n",
    "        print(f\"Image types: {list(adata.uns['spatial'][first_slide]['images'].keys())}\")\n",
    "        \n",
    "        # Check image content\n",
    "        for img_type in adata.uns['spatial'][first_slide]['images']:\n",
    "            img = adata.uns['spatial'][first_slide]['images'][img_type]\n",
    "            \n",
    "            # Handle different types of data\n",
    "            if isinstance(img, np.ndarray):\n",
    "                print(f\"  {img_type} shape: {img.shape}\")\n",
    "                print(f\"  Data type: {img.dtype}\")\n",
    "                print(f\"  Min/Max values: {img.min()}, {img.max()}\")\n",
    "            elif isinstance(img, str):\n",
    "                print(f\"  {img_type}: String value - '{img}'\")\n",
    "            else:\n",
    "                print(f\"  {img_type}: {type(img).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eee209c",
   "metadata": {},
   "source": [
    "### create a data generator that extracts image patches from the AnnData's high-resolution images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27e1050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds_from_anndata(indices, batch=32, shuffle_buffer=10000):\n",
    "    \"\"\"Extract image patches from AnnData instead of HDF5\"\"\"\n",
    "    def gen():\n",
    "        for i in indices:\n",
    "            slide = slide_ids[i]\n",
    "            \n",
    "            # Get high-resolution image from AnnData\n",
    "            hires_img = adata.uns['spatial'][slide]['images']['hires']\n",
    "            \n",
    "            # Get coordinates for this spot\n",
    "            # First find which spatial coordinate matrix to use\n",
    "            spatial_key = f'spatial_{slide}'\n",
    "            if spatial_key not in adata.obsm:\n",
    "                # Try alternative key formats if needed\n",
    "                spatial_key = 'X_spatial'\n",
    "            \n",
    "            # Determine which index within this slide's spots corresponds to this spot\n",
    "            slide_spots = np.where(slide_ids == slide)[0]\n",
    "            local_idx = np.where(slide_spots == i)[0][0]\n",
    "            \n",
    "            # Get spot coordinates\n",
    "            spot_coord = adata.obsm[spatial_key][local_idx]\n",
    "            \n",
    "            # Scale factors might be needed to convert between spot coordinates and pixel coordinates\n",
    "            scale = adata.uns['spatial'][slide]['scalefactors'].get('tissue_hires_scalef', 1.0)\n",
    "            \n",
    "            # Convert to pixel coordinates\n",
    "            x, y = int(spot_coord[0] * scale), int(spot_coord[1] * scale)\n",
    "            \n",
    "            # Extract patch (224x224)\n",
    "            patch_size = 224\n",
    "            half_size = patch_size // 2\n",
    "            \n",
    "            # Handle edge cases\n",
    "            if (y - half_size < 0 or x - half_size < 0 or \n",
    "                y + half_size >= hires_img.shape[0] or \n",
    "                x + half_size >= hires_img.shape[1]):\n",
    "                # Create a blank patch for spots near edges\n",
    "                patch = np.zeros((patch_size, patch_size, 3), dtype=np.float32)\n",
    "                \n",
    "                # Try to copy what portion is available\n",
    "                y_start = max(0, y - half_size)\n",
    "                y_end = min(hires_img.shape[0], y + half_size)\n",
    "                x_start = max(0, x - half_size)\n",
    "                x_end = min(hires_img.shape[1], x + half_size)\n",
    "                \n",
    "                # Calculate where to place in our patch\n",
    "                patch_y_start = max(0, half_size - y)\n",
    "                patch_x_start = max(0, half_size - x)\n",
    "                \n",
    "                # Copy available portion of the image\n",
    "                patch_y_end = patch_y_start + (y_end - y_start)\n",
    "                patch_x_end = patch_x_start + (x_end - x_start)\n",
    "                \n",
    "                patch[patch_y_start:patch_y_end, patch_x_start:patch_x_end] = hires_img[y_start:y_end, x_start:x_end]\n",
    "            else:\n",
    "                # Standard case - extract full patch\n",
    "                patch = hires_img[y-half_size:y+half_size, x-half_size:x+half_size]\n",
    "            \n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"img\": patch,\n",
    "                \"gene\": genes_all[i],\n",
    "                \"xy\": xy_all[i],\n",
    "                \"slide\": np.array([slide_codes[i]], dtype=np.int32),\n",
    "                \"sample_type\": np.array([sample_type_vec[i]], dtype=np.int32),\n",
    "            }\n",
    "            targets = {\n",
    "                \"class_out\": y_all[i],\n",
    "                \"type_out\": tf.one_hot(sample_type_vec[i], 4),\n",
    "            }\n",
    "            yield inputs, targets\n",
    "    \n",
    "    # Same signature as original generator\n",
    "    out_sig = (\n",
    "        {\"img\": tf.TensorSpec((224,224,3), tf.float32),\n",
    "         \"gene\": tf.TensorSpec((genes_all.shape[1],), tf.float32),\n",
    "         \"xy\": tf.TensorSpec((2,), tf.float32),\n",
    "         \"slide\": tf.TensorSpec((1,), tf.int32),\n",
    "         \"sample_type\": tf.TensorSpec((1,), tf.int32)},\n",
    "        {\"class_out\": tf.TensorSpec((num_classes,), tf.float32),\n",
    "         \"type_out\": tf.TensorSpec((4,), tf.float32)}\n",
    "    )\n",
    "    \n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=out_sig)\n",
    "    \n",
    "    if shuffle_buffer is not None:\n",
    "        ds = ds.shuffle(shuffle_buffer)\n",
    "        \n",
    "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d668872",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30f77acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 73196/73196 spots have valid coordinates\n",
      "Validation: 18300/18300 spots have valid coordinates\n"
     ]
    }
   ],
   "source": [
    "def filter_valid_indices(indices):\n",
    "    \"\"\"Filter indices to keep only those with valid spatial coordinates\"\"\"\n",
    "    valid_indices = []\n",
    "    \n",
    "    # Create map of valid coordinate indices for each slide\n",
    "    valid_coords = {}\n",
    "    for slide in adata.uns['spatial'].keys():\n",
    "        spatial_key = f'spatial_{slide}'\n",
    "        valid_mask = ~np.isnan(adata.obsm[spatial_key]).any(axis=1)\n",
    "        valid_coords[slide] = set(np.where(valid_mask)[0])\n",
    "    \n",
    "    # Filter indices\n",
    "    for i in indices:\n",
    "        slide = slide_ids[i]\n",
    "        if i in valid_coords[slide]:\n",
    "            valid_indices.append(i)\n",
    "    \n",
    "    return np.array(valid_indices)\n",
    "\n",
    "# Filter training and validation indices\n",
    "valid_train_indices = filter_valid_indices(idx_train)\n",
    "valid_val_indices = filter_valid_indices(idx_val)\n",
    "\n",
    "print(f\"Training: {len(valid_train_indices)}/{len(idx_train)} spots have valid coordinates\")\n",
    "print(f\"Validation: {len(valid_val_indices)}/{len(idx_val)} spots have valid coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c80f8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds_with_images(indices, batch=32, shuffle_buffer=10000):\n",
    "    \"\"\"Data generator that properly handles the sparse spatial coordinate structure\"\"\"\n",
    "    \n",
    "    # Create map of valid coordinate indices for each slide\n",
    "    valid_coords = {}\n",
    "    for slide in adata.uns['spatial'].keys():\n",
    "        spatial_key = f'spatial_{slide}'\n",
    "        valid_mask = ~np.isnan(adata.obsm[spatial_key]).any(axis=1)\n",
    "        valid_coords[slide] = set(np.where(valid_mask)[0])\n",
    "    \n",
    "    def gen():\n",
    "        for i in indices:\n",
    "            slide = slide_ids[i]\n",
    "            \n",
    "            # Check if this index has valid coordinates for its slide\n",
    "            if i in valid_coords[slide]:\n",
    "                # Has valid coordinates - extract image patch\n",
    "                spot_coord = adata.obsm[f'spatial_{slide}'][i]\n",
    "                \n",
    "                # Get image and scale factor\n",
    "                hires_img = adata.uns['spatial'][slide]['images']['hires']\n",
    "                scale = adata.uns['spatial'][slide]['scalefactors'].get('tissue_hires_scalef', 1.0)\n",
    "                \n",
    "                # Convert to pixel coordinates\n",
    "                x, y = int(spot_coord[0] * scale), int(spot_coord[1] * scale)\n",
    "                \n",
    "                # Extract patch (224x224)\n",
    "                patch_size = 224\n",
    "                half_size = patch_size // 2\n",
    "                \n",
    "                # Create empty patch of correct size\n",
    "                patch = np.zeros((patch_size, patch_size, 3), dtype=np.float32)\n",
    "                \n",
    "                # Calculate source and destination coordinates\n",
    "                # Source (in the original image)\n",
    "                src_y_start = max(0, y - half_size)\n",
    "                src_y_end = min(hires_img.shape[0], y + half_size)\n",
    "                src_x_start = max(0, x - half_size)\n",
    "                src_x_end = min(hires_img.shape[1], x + half_size)\n",
    "                \n",
    "                # Destination (in our patch)\n",
    "                dst_y_start = max(0, half_size - y)\n",
    "                dst_y_end = dst_y_start + (src_y_end - src_y_start)\n",
    "                dst_x_start = max(0, half_size - x)\n",
    "                dst_x_end = dst_x_start + (src_x_end - src_x_start)\n",
    "                \n",
    "                # Only copy if we have valid dimensions (both width and height > 0)\n",
    "                if (src_y_end > src_y_start) and (src_x_end > src_x_start) and \\\n",
    "                   (dst_y_end > dst_y_start) and (dst_x_end > dst_x_start):\n",
    "                    # Copy available portion of the image\n",
    "                    patch[dst_y_start:dst_y_end, dst_x_start:dst_x_end] = \\\n",
    "                        hires_img[src_y_start:src_y_end, src_x_start:src_x_end]\n",
    "            else:\n",
    "                # No valid coordinates - use a blank patch\n",
    "                patch = np.zeros((224, 224, 3), dtype=np.float32)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"img\": patch,\n",
    "                \"gene\": genes_all[i],\n",
    "                \"xy\": xy_all[i],\n",
    "                \"slide\": np.array([slide_codes[i]], dtype=np.int32),\n",
    "                \"sample_type\": np.array([sample_type_vec[i]], dtype=np.int32),\n",
    "            }\n",
    "            targets = {\n",
    "                \"class_out\": y_all[i],\n",
    "                \"type_out\": tf.one_hot(sample_type_vec[i], 4),\n",
    "            }\n",
    "            yield inputs, targets\n",
    "    \n",
    "    # Output signature\n",
    "    out_sig = (\n",
    "        {\"img\": tf.TensorSpec((224,224,3), tf.float32),\n",
    "         \"gene\": tf.TensorSpec((genes_all.shape[1],), tf.float32),\n",
    "         \"xy\": tf.TensorSpec((2,), tf.float32),\n",
    "         \"slide\": tf.TensorSpec((1,), tf.int32),\n",
    "         \"sample_type\": tf.TensorSpec((1,), tf.int32)},\n",
    "        {\"class_out\": tf.TensorSpec((num_classes,), tf.float32),\n",
    "         \"type_out\": tf.TensorSpec((4,), tf.float32)}\n",
    "    )\n",
    "    \n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=out_sig)\n",
    "    \n",
    "    if shuffle_buffer is not None:\n",
    "        ds = ds.shuffle(shuffle_buffer)\n",
    "        \n",
    "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1a91437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Compilation:\n",
    "optimizer = Adam(learning_rate=0.0001, clipnorm=1.0)\n",
    "\n",
    "# Compile with the improved optimizer\n",
    "model_robust.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\"class_out\": \"categorical_crossentropy\",\n",
    "          \"type_out\": \"categorical_crossentropy\"},\n",
    "    loss_weights={\"class_out\": 1.0, \"type_out\": 0.3},\n",
    "    metrics={\"class_out\": \"accuracy\", \"type_out\": \"accuracy\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64eabbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img (InputLayer)                [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 224, 224, 32) 896         img[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "gene (InputLayer)               [(None, 17860)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 224, 224, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          9144832     gene[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 224, 224, 32) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 512)          2048        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 112, 112, 32) 0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 112, 112, 64) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "xy (InputLayer)                 [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 112, 112, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          65664       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           96          xy[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "slide (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sample_type (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 112, 112, 64) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128)          512         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32)           128         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 8)         240         slide[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 4)         16          sample_type[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 64)           0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32)           0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 236)          0           global_average_pooling2d_1[0][0] \n",
      "                                                                 activation_9[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 236)          944         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          30336       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128)          512         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 128)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 64)           8256        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "class_out (Dense)               (None, 2)            130         latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "type_out (Dense)                (None, 4)            260         latent[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 9,273,750\n",
      "Trainable params: 9,271,486\n",
      "Non-trainable params: 2,264\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define inputs\n",
    "img_in = Input((224,224,3), name=\"img\")\n",
    "gene_in = Input((genes_all.shape[1],), name=\"gene\")\n",
    "xy_in = Input((2,), name=\"xy\")\n",
    "slide_in = Input((1,), name=\"slide\")\n",
    "st_in = Input((1,), name=\"sample_type\")\n",
    "\n",
    "# Image tower with batch normalization\n",
    "h = layers.Conv2D(32, 3, padding='same')(img_in)\n",
    "h = layers.BatchNormalization()(h)\n",
    "h = layers.Activation(\"relu\")(h)\n",
    "h = layers.MaxPool2D()(h)\n",
    "h = layers.Conv2D(64, 3, padding='same')(h)\n",
    "h = layers.BatchNormalization()(h)\n",
    "h = layers.Activation(\"relu\")(h)\n",
    "h = layers.GlobalAveragePooling2D()(h)\n",
    "\n",
    "# Gene tower with batch normalization\n",
    "g = layers.Dense(512)(gene_in)  # REMOVED regularizer\n",
    "g = layers.BatchNormalization()(g)\n",
    "g = layers.Activation(\"relu\")(g)\n",
    "g = layers.Dropout(0.3)(g)  # CHANGED from 0.5 to 0.3\n",
    "g = layers.Dense(128)(g)\n",
    "g = layers.BatchNormalization()(g)\n",
    "g = layers.Activation(\"relu\")(g)\n",
    "\n",
    "# Coordinate tower\n",
    "s = layers.Dense(32)(xy_in)\n",
    "s = layers.BatchNormalization()(s)\n",
    "s = layers.Activation(\"relu\")(s)\n",
    "\n",
    "# Slide-ID embedding\n",
    "b = layers.Flatten()(layers.Embedding(n_slides, 8)(slide_in))\n",
    "\n",
    "# Sample-type embedding\n",
    "st = layers.Flatten()(layers.Embedding(4, 4)(st_in))\n",
    "\n",
    "# Fusion layer with batch normalization\n",
    "z = layers.concatenate([h, g, s, b, st])\n",
    "z = layers.BatchNormalization()(z)\n",
    "z = layers.Dense(128)(z)\n",
    "z = layers.BatchNormalization()(z)\n",
    "z = layers.Activation(\"relu\")(z)\n",
    "z_lat = layers.Dense(64, activation=\"relu\", name=\"latent\")(z)\n",
    "\n",
    "# Classification heads\n",
    "class_out = layers.Dense(num_classes, activation=\"softmax\", name=\"class_out\")(z_lat)\n",
    "type_out = layers.Dense(4, activation=\"softmax\", name=\"type_out\")(z_lat)  # SIMPLIFIED\n",
    "\n",
    "# Assemble model\n",
    "model_robust = Model(\n",
    "    {\"img\": img_in, \"gene\": gene_in, \"xy\": xy_in,\n",
    "     \"slide\": slide_in, \"sample_type\": st_in},\n",
    "    {\"class_out\": class_out, \"type_out\": type_out}\n",
    ")\n",
    "\n",
    "# Compile with CORRECT learning rate (0.0001 not 0.001)\n",
    "optimizer = Adam(learning_rate=0.0001, clipnorm=1.0)  # CHANGED from 0.001\n",
    "model_robust.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\"class_out\": \"categorical_crossentropy\",\n",
    "          \"type_out\": \"categorical_crossentropy\"},\n",
    "    loss_weights={\"class_out\": 1.0, \"type_out\": 0.3},\n",
    "    metrics={\"class_out\": \"accuracy\", \"type_out\": \"accuracy\"}\n",
    ")\n",
    "\n",
    "model_robust.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f80c5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds_with_images(indices, batch=32, shuffle_buffer=10000):\n",
    "    \"\"\"Data generator with improved numerical stability\"\"\"\n",
    "    \n",
    "    # Create map of valid coordinate indices for each slide\n",
    "    valid_coords = {}\n",
    "    for slide in adata.uns['spatial'].keys():\n",
    "        spatial_key = f'spatial_{slide}'\n",
    "        valid_mask = ~np.isnan(adata.obsm[spatial_key]).any(axis=1)\n",
    "        valid_coords[slide] = set(np.where(valid_mask)[0])\n",
    "    \n",
    "    def gen():\n",
    "        for i in indices:\n",
    "            slide = slide_ids[i]\n",
    "            \n",
    "            # Create empty patch of correct size with all zeros\n",
    "            patch = np.zeros((224, 224, 3), dtype=np.float32)\n",
    "            \n",
    "            # Check if this index has valid coordinates for its slide\n",
    "            if i in valid_coords[slide]:\n",
    "                try:\n",
    "                    # Has valid coordinates - extract image patch\n",
    "                    spot_coord = adata.obsm[f'spatial_{slide}'][i]\n",
    "                    \n",
    "                    # Get image and scale factor\n",
    "                    hires_img = adata.uns['spatial'][slide]['images']['hires']\n",
    "                    \n",
    "                    # Ensure image is in range [0,1]\n",
    "                    if hires_img.max() > 1.0:\n",
    "                        hires_img = hires_img / 255.0\n",
    "                    \n",
    "                    scale = adata.uns['spatial'][slide]['scalefactors'].get('tissue_hires_scalef', 1.0)\n",
    "                    \n",
    "                    # Convert to pixel coordinates\n",
    "                    x, y = int(spot_coord[0] * scale), int(spot_coord[1] * scale)\n",
    "                    \n",
    "                    # Extract patch (224x224)\n",
    "                    patch_size = 224\n",
    "                    half_size = patch_size // 2\n",
    "                    \n",
    "                    # Calculate source and destination coordinates\n",
    "                    # Source (in the original image)\n",
    "                    src_y_start = max(0, y - half_size)\n",
    "                    src_y_end = min(hires_img.shape[0], y + half_size)\n",
    "                    src_x_start = max(0, x - half_size)\n",
    "                    src_x_end = min(hires_img.shape[1], x + half_size)\n",
    "                    \n",
    "                    # Destination (in our patch)\n",
    "                    dst_y_start = max(0, half_size - y)\n",
    "                    dst_y_end = dst_y_start + (src_y_end - src_y_start)\n",
    "                    dst_x_start = max(0, half_size - x)\n",
    "                    dst_x_end = dst_x_start + (src_x_end - src_x_start)\n",
    "                    \n",
    "                    # Only copy if we have valid dimensions\n",
    "                    if (src_y_end > src_y_start) and (src_x_end > src_x_start) and \\\n",
    "                       (dst_y_end > dst_y_start) and (dst_x_end > dst_x_start):\n",
    "                        patch[dst_y_start:dst_y_end, dst_x_start:dst_x_end] = \\\n",
    "                            hires_img[src_y_start:src_y_end, src_x_start:src_x_end]\n",
    "                except Exception as e:\n",
    "                    # If anything goes wrong, just use a blank patch\n",
    "                    print(f\"Error extracting patch: {e}\")\n",
    "            \n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                \"img\": patch,\n",
    "                \"gene\": genes_all[i],\n",
    "                \"xy\": xy_all[i],\n",
    "                \"slide\": np.array([slide_codes[i]], dtype=np.int32),\n",
    "                \"sample_type\": np.array([sample_type_vec[i]], dtype=np.int32),\n",
    "            }\n",
    "            targets = {\n",
    "                \"class_out\": y_all[i],\n",
    "                \"type_out\": tf.one_hot(sample_type_vec[i], 4),\n",
    "            }\n",
    "            yield inputs, targets\n",
    "    \n",
    "    # Output signature\n",
    "    out_sig = (\n",
    "        {\"img\": tf.TensorSpec((224,224,3), tf.float32),\n",
    "         \"gene\": tf.TensorSpec((genes_all.shape[1],), tf.float32),\n",
    "         \"xy\": tf.TensorSpec((2,), tf.float32),\n",
    "         \"slide\": tf.TensorSpec((1,), tf.int32),\n",
    "         \"sample_type\": tf.TensorSpec((1,), tf.int32)},\n",
    "        {\"class_out\": tf.TensorSpec((num_classes,), tf.float32),\n",
    "         \"type_out\": tf.TensorSpec((4,), tf.float32)}\n",
    "    )\n",
    "    \n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=out_sig)\n",
    "    \n",
    "    if shuffle_buffer is not None:\n",
    "        ds = ds.shuffle(shuffle_buffer)\n",
    "        \n",
    "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b558ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_weights_only=True  # This helps avoid errors\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=False  # Changed to False to avoid errors\n",
    "    ),\n",
    "    # Add this to catch NaN and stop training immediately\n",
    "    tf.keras.callbacks.TerminateOnNaN()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a034bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create train/validation split of your data indices\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'indices' is the full set of available data indices\n",
    "# If you don't have this defined, create it:\n",
    "indices = np.arange(len(genes_all))  # Or however many samples you have\n",
    "\n",
    "\n",
    "# Split indices into train and validation sets\n",
    "train_indices, val_indices = train_test_split(\n",
    "    indices, test_size=0.2, random_state=42, \n",
    "    stratify=sample_type_vec if 'sample_type_vec' in globals() else None\n",
    ")\n",
    "\n",
    "# Create datasets using these indices\n",
    "train_ds = make_ds_with_images(train_indices, batch=32)\n",
    "val_ds = make_ds_with_images(val_indices, batch=32, shuffle_buffer=None)\n",
    "\n",
    "\n",
    "# Now the rest of your code will work because train_ds and val_ds are defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03109c4b",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76011e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2288/2288 [==============================] - 477s 173ms/step - loss: 0.9847 - class_out_loss: 0.5439 - type_out_loss: 1.4690 - class_out_accuracy: 0.7549 - type_out_accuracy: 0.2883 - val_loss: 0.8471 - val_class_out_loss: 0.4106 - val_type_out_loss: 1.4547 - val_class_out_accuracy: 0.9006 - val_type_out_accuracy: 0.2769\n",
      "Epoch 2/50\n",
      "2288/2288 [==============================] - 85s 37ms/step - loss: 0.9851 - class_out_loss: 0.5446 - type_out_loss: 1.4682 - class_out_accuracy: 0.7523 - type_out_accuracy: 0.2860 - val_loss: 0.8470 - val_class_out_loss: 0.4108 - val_type_out_loss: 1.4539 - val_class_out_accuracy: 0.9001 - val_type_out_accuracy: 0.2793\n",
      "Epoch 3/50\n",
      "2288/2288 [==============================] - 79s 34ms/step - loss: 0.9847 - class_out_loss: 0.5435 - type_out_loss: 1.4706 - class_out_accuracy: 0.7540 - type_out_accuracy: 0.2856 - val_loss: 0.8476 - val_class_out_loss: 0.4113 - val_type_out_loss: 1.4544 - val_class_out_accuracy: 0.9008 - val_type_out_accuracy: 0.2773\n",
      "Epoch 4/50\n",
      "2288/2288 [==============================] - 75s 33ms/step - loss: 0.9859 - class_out_loss: 0.5453 - type_out_loss: 1.4686 - class_out_accuracy: 0.7525 - type_out_accuracy: 0.2908 - val_loss: 0.8470 - val_class_out_loss: 0.4108 - val_type_out_loss: 1.4537 - val_class_out_accuracy: 0.9007 - val_type_out_accuracy: 0.2777\n",
      "Epoch 5/50\n",
      "2288/2288 [==============================] - 78s 34ms/step - loss: 0.9849 - class_out_loss: 0.5439 - type_out_loss: 1.4696 - class_out_accuracy: 0.7566 - type_out_accuracy: 0.2897 - val_loss: 0.8493 - val_class_out_loss: 0.4123 - val_type_out_loss: 1.4562 - val_class_out_accuracy: 0.8990 - val_type_out_accuracy: 0.2758\n",
      "Epoch 6/50\n",
      "2288/2288 [==============================] - 77s 34ms/step - loss: 0.9859 - class_out_loss: 0.5445 - type_out_loss: 1.4710 - class_out_accuracy: 0.7572 - type_out_accuracy: 0.2888 - val_loss: 0.8501 - val_class_out_loss: 0.4142 - val_type_out_loss: 1.4530 - val_class_out_accuracy: 0.8980 - val_type_out_accuracy: 0.2790\n",
      "Epoch 7/50\n",
      "2288/2288 [==============================] - 81s 35ms/step - loss: 0.9838 - class_out_loss: 0.5431 - type_out_loss: 1.4686 - class_out_accuracy: 0.7542 - type_out_accuracy: 0.2929 - val_loss: 0.8465 - val_class_out_loss: 0.4099 - val_type_out_loss: 1.4550 - val_class_out_accuracy: 0.9008 - val_type_out_accuracy: 0.2786\n",
      "Epoch 8/50\n",
      "2288/2288 [==============================] - 81s 35ms/step - loss: 0.9860 - class_out_loss: 0.5447 - type_out_loss: 1.4708 - class_out_accuracy: 0.7547 - type_out_accuracy: 0.2871 - val_loss: 0.8481 - val_class_out_loss: 0.4112 - val_type_out_loss: 1.4562 - val_class_out_accuracy: 0.9005 - val_type_out_accuracy: 0.2756\n",
      "Epoch 9/50\n",
      "2288/2288 [==============================] - 80s 35ms/step - loss: 0.9854 - class_out_loss: 0.5448 - type_out_loss: 1.4682 - class_out_accuracy: 0.7546 - type_out_accuracy: 0.2901 - val_loss: 0.8483 - val_class_out_loss: 0.4119 - val_type_out_loss: 1.4545 - val_class_out_accuracy: 0.8994 - val_type_out_accuracy: 0.2779\n",
      "Epoch 10/50\n",
      "2288/2288 [==============================] - 83s 36ms/step - loss: 0.9864 - class_out_loss: 0.5454 - type_out_loss: 1.4698 - class_out_accuracy: 0.7551 - type_out_accuracy: 0.2916 - val_loss: 0.8482 - val_class_out_loss: 0.4111 - val_type_out_loss: 1.4565 - val_class_out_accuracy: 0.9014 - val_type_out_accuracy: 0.2759\n",
      "Epoch 11/50\n",
      "2288/2288 [==============================] - 77s 34ms/step - loss: 0.9841 - class_out_loss: 0.5429 - type_out_loss: 1.4705 - class_out_accuracy: 0.7552 - type_out_accuracy: 0.2898 - val_loss: 0.8460 - val_class_out_loss: 0.4098 - val_type_out_loss: 1.4538 - val_class_out_accuracy: 0.9013 - val_type_out_accuracy: 0.2790\n",
      "Epoch 12/50\n",
      "2288/2288 [==============================] - 75s 33ms/step - loss: 0.9844 - class_out_loss: 0.5439 - type_out_loss: 1.4681 - class_out_accuracy: 0.7559 - type_out_accuracy: 0.2888 - val_loss: 0.8467 - val_class_out_loss: 0.4106 - val_type_out_loss: 1.4537 - val_class_out_accuracy: 0.9007 - val_type_out_accuracy: 0.2773\n",
      "Epoch 13/50\n",
      "2288/2288 [==============================] - 76s 33ms/step - loss: 0.9859 - class_out_loss: 0.5443 - type_out_loss: 1.4717 - class_out_accuracy: 0.7543 - type_out_accuracy: 0.2891 - val_loss: 0.8482 - val_class_out_loss: 0.4117 - val_type_out_loss: 1.4548 - val_class_out_accuracy: 0.9004 - val_type_out_accuracy: 0.2779\n",
      "Epoch 14/50\n",
      "2288/2288 [==============================] - 76s 33ms/step - loss: 0.9880 - class_out_loss: 0.5462 - type_out_loss: 1.4724 - class_out_accuracy: 0.7541 - type_out_accuracy: 0.2857 - val_loss: 0.8470 - val_class_out_loss: 0.4106 - val_type_out_loss: 1.4546 - val_class_out_accuracy: 0.9011 - val_type_out_accuracy: 0.2778\n",
      "Epoch 15/50\n",
      "2288/2288 [==============================] - 76s 33ms/step - loss: 0.9866 - class_out_loss: 0.5454 - type_out_loss: 1.4705 - class_out_accuracy: 0.7526 - type_out_accuracy: 0.2867 - val_loss: 0.8480 - val_class_out_loss: 0.4116 - val_type_out_loss: 1.4545 - val_class_out_accuracy: 0.9002 - val_type_out_accuracy: 0.2773\n",
      "Epoch 16/50\n",
      "2288/2288 [==============================] - 74s 32ms/step - loss: 0.9859 - class_out_loss: 0.5452 - type_out_loss: 1.4688 - class_out_accuracy: 0.7521 - type_out_accuracy: 0.2903 - val_loss: 0.8455 - val_class_out_loss: 0.4092 - val_type_out_loss: 1.4540 - val_class_out_accuracy: 0.9013 - val_type_out_accuracy: 0.2785\n",
      "Epoch 17/50\n",
      "2288/2288 [==============================] - 72s 31ms/step - loss: 0.9835 - class_out_loss: 0.5432 - type_out_loss: 1.4675 - class_out_accuracy: 0.7561 - type_out_accuracy: 0.2856 - val_loss: 0.8440 - val_class_out_loss: 0.4078 - val_type_out_loss: 1.4540 - val_class_out_accuracy: 0.9030 - val_type_out_accuracy: 0.2772\n",
      "Epoch 18/50\n",
      "2288/2288 [==============================] - 74s 32ms/step - loss: 0.9833 - class_out_loss: 0.5427 - type_out_loss: 1.4686 - class_out_accuracy: 0.7556 - type_out_accuracy: 0.2882 - val_loss: 0.8462 - val_class_out_loss: 0.4098 - val_type_out_loss: 1.4543 - val_class_out_accuracy: 0.9010 - val_type_out_accuracy: 0.2769\n",
      "Epoch 19/50\n",
      "2288/2288 [==============================] - 74s 32ms/step - loss: 0.9843 - class_out_loss: 0.5438 - type_out_loss: 1.4682 - class_out_accuracy: 0.7544 - type_out_accuracy: 0.2901 - val_loss: 0.8474 - val_class_out_loss: 0.4109 - val_type_out_loss: 1.4547 - val_class_out_accuracy: 0.9010 - val_type_out_accuracy: 0.2768\n",
      "Epoch 20/50\n",
      "2288/2288 [==============================] - 73s 32ms/step - loss: 0.9863 - class_out_loss: 0.5448 - type_out_loss: 1.4714 - class_out_accuracy: 0.7551 - type_out_accuracy: 0.2865 - val_loss: 0.8470 - val_class_out_loss: 0.4105 - val_type_out_loss: 1.4549 - val_class_out_accuracy: 0.9013 - val_type_out_accuracy: 0.2770\n",
      "Epoch 21/50\n",
      "2288/2288 [==============================] - 75s 33ms/step - loss: 0.9850 - class_out_loss: 0.5443 - type_out_loss: 1.4689 - class_out_accuracy: 0.7562 - type_out_accuracy: 0.2906 - val_loss: 0.8463 - val_class_out_loss: 0.4105 - val_type_out_loss: 1.4526 - val_class_out_accuracy: 0.9010 - val_type_out_accuracy: 0.2764\n",
      "Epoch 22/50\n",
      "2288/2288 [==============================] - 73s 32ms/step - loss: 0.9861 - class_out_loss: 0.5443 - type_out_loss: 1.4722 - class_out_accuracy: 0.7539 - type_out_accuracy: 0.2873 - val_loss: 0.8449 - val_class_out_loss: 0.4090 - val_type_out_loss: 1.4528 - val_class_out_accuracy: 0.9020 - val_type_out_accuracy: 0.2783\n",
      "Epoch 23/50\n",
      "2288/2288 [==============================] - 74s 32ms/step - loss: 0.9844 - class_out_loss: 0.5440 - type_out_loss: 1.4679 - class_out_accuracy: 0.7561 - type_out_accuracy: 0.2900 - val_loss: 0.8463 - val_class_out_loss: 0.4107 - val_type_out_loss: 1.4519 - val_class_out_accuracy: 0.9005 - val_type_out_accuracy: 0.2802\n",
      "Epoch 24/50\n",
      "2288/2288 [==============================] - 73s 32ms/step - loss: 0.9850 - class_out_loss: 0.5438 - type_out_loss: 1.4704 - class_out_accuracy: 0.7552 - type_out_accuracy: 0.2891 - val_loss: 0.8454 - val_class_out_loss: 0.4088 - val_type_out_loss: 1.4550 - val_class_out_accuracy: 0.9022 - val_type_out_accuracy: 0.2767\n",
      "Epoch 25/50\n",
      "2288/2288 [==============================] - 74s 32ms/step - loss: 0.9848 - class_out_loss: 0.5444 - type_out_loss: 1.4677 - class_out_accuracy: 0.7553 - type_out_accuracy: 0.2903 - val_loss: 0.8481 - val_class_out_loss: 0.4114 - val_type_out_loss: 1.4556 - val_class_out_accuracy: 0.9002 - val_type_out_accuracy: 0.2772\n",
      "Epoch 26/50\n",
      "2288/2288 [==============================] - 75s 33ms/step - loss: 0.9851 - class_out_loss: 0.5439 - type_out_loss: 1.4705 - class_out_accuracy: 0.7560 - type_out_accuracy: 0.2888 - val_loss: 0.8460 - val_class_out_loss: 0.4100 - val_type_out_loss: 1.4532 - val_class_out_accuracy: 0.9020 - val_type_out_accuracy: 0.2799\n",
      "Epoch 27/50\n",
      "2288/2288 [==============================] - 74s 32ms/step - loss: 0.9852 - class_out_loss: 0.5446 - type_out_loss: 1.4682 - class_out_accuracy: 0.7523 - type_out_accuracy: 0.2868 - val_loss: 0.8475 - val_class_out_loss: 0.4116 - val_type_out_loss: 1.4527 - val_class_out_accuracy: 0.8999 - val_type_out_accuracy: 0.2790\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GPU optimization\n",
    "if isinstance(train_ds, tf.data.Dataset):\n",
    "    train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "if isinstance(val_ds, tf.data.Dataset):\n",
    "    val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train the model - NO class_weight parameter!\n",
    "history = model_robust.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    workers=4,\n",
    "    use_multiprocessing=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# After training, you can save the model\n",
    "model_robust.save('final_model.h5')\n",
    "\n",
    "# You can also save the training history\n",
    "import pandas as pd\n",
    "pd.DataFrame(history.history).to_csv('training_history.csv', index=False)\n",
    "\n",
    "# If you want to visualize the training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['class_out_accuracy'])\n",
    "plt.plot(history.history['val_class_out_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
